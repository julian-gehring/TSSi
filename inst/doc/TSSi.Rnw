%% LyX 2.0.0 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[a4paper,english]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=true,pdfborder={0 0 0},backref=false,colorlinks=false]
 {hyperref}
\usepackage{breakurl}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\special{papersize=\the\paperwidth,\the\paperheight}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
<<echo=F>>=
  if(exists("ls.enc")) options(encoding=ls.enc)
@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\VignetteIndexEntry{Introduction to the TSSi package: Identification of Transcription Start Sites}
%\VignettePackage{TSSi}

\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\textit{#1}}}
\newcommand{\Rclass}[1]{{\textit{#1}}}
\newcommand{\Rmethod}[1]{{\textit{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}
\newcommand{\Rvar}[1]{{\textit{\textsf{#1}}}}

%% avoid single lines
\widowpenalty=10000
\clubpenalty=10000

%% format captions
\usepackage[small,bf,margin=.5cm]{caption}

\makeatother

\begin{document}

\title{Introduction to the \Rpackage{TSSi} package:\\
Identification of Transcription Start Sites}


\author{Julian Gehring, Clemens Kreutz}

\maketitle
<<settings, echo=FALSE>>=
set.seed(1)
options(width=65, SweaveHooks=list(fig=function() par(mar=c(5.1, 5.1, 4.1, 1.5))))
@


\section{Introduction}

Along with the advances in high-throughput sequencing, the detection
of transcription start sites \emph{(TSS)} using CAP-capture techniques
has evolved recently. While many experimental applications exist,
the analysis of such data is still non-trivial. Approaching this,
the \Rpackage{TSSi} package offers a flexible statistical preprocessing
for CAP-capture data and an automated identification of start sites.


\section{Data set}

In this vignette we use experimental CAP-capture data obtained with
Solexa sequencing. The data was mapped to the genome using the bowtie
algorithm and processed, such that the number of the 5' end of reads
for each position are available. The data frame \Rvar{readData} contains
the chromosome, the strand, the 5' position of the reads, and the
total number of reads at that position. Further, regions based on
existing annotation are also provided which are used to divide the
data into independent subsets for analysis.

<<load_data>>=
library(TSSi)
data(readData)
head(readData)
@


\section{Segment read data}

As a first step in the analysis, the read data is taken by the \Rmethod{segmentizeCounts}
method. Here, the data is divided into \emph{segments}, for which
the following analysis is performed independently. This is done based
on the information of the chromosomes, the strands, and the optional
regions. The segmented data is returned as an object of the class
\Rclass{TssData}.

<<segmentize>>=
attach(readData)
x <- segmentizeCounts(counts=counts, start=start, chr=chromosome, region=region, strand=strand)
detach(readData)
x
@

The final segments and the associated read data can be assessed with
several \Rmethod{get} methods. The data from individual segments
can be called either by its name or an index. Each segment can easily
be visualized with the \Rmethod{plot} method.


<<get_segments>>=
segments(x)
names(x)
@

<<get_reads>>=
head(reads(x, 3))
head(start(x, 3))
head(start(x, names(x)[3]))
@


\section{Normalization}

The normalization reduces the noise by shrinking the counts towards
zero. This step is intended to eliminate false positive counts as
well as making further analyzes more robust by reducing the impact
of large counts. Such a shrinkage or regularization procedure constitutes
a well-established strategy in statistics to make predictions conservative,
i.e. to reduce the number of false positive predictions. To enhance
the shrinkage of isolated counts in comparison to counts in regions
of strong transcriptional activity, the information of consecutive
genomic positions in the measurements is regarded by evaluating differences
between adjacent count estimates.

The computation can be performed with a fast approximation of the
distribution based on all reads, or fitted explicitly for each segment.
On platforms supporting the \Rpackage{multicore} package, the fitting
can be spread over multiple processor cores in order to decrease computation
time.

<<normalize_ratio>>=
yRatio <- normalizeCounts(x)
@

<<normalize_fit>>=
yFit <- normalizeCounts(x, fit=TRUE)
yFit
head(reads(yFit, 3))
@

<<plot_normalize_fit, fig=TRUE, echo=TRUE>>=
plot(yFit, 3)
@


\section{Identifying transcription start sites}

After normalization of the count data, an iterative algorithm is applied
for each segment to identify the TSS. The expected number of false
positive counts is initialized with a default value given by the read
frequency in the whole data set. The position with the largest counts
above is identified as a TSS, if the expected transcription level
is at least one read above the expected number of false positive reads.
The transcription levels for all TSS are calculated by adding all
counts to their nearest neighbor TSS.

Then, the expected number of false positive reads is updated by convolution
with exponential kernels. The decay rates \Rfunarg{tau} in 3' direction
and towards the 5'-end can be chosen differently to account for the
fact that false positive counts are preferably found in 5' direction
of a TSS. This procedure is iterated as long as the set of TSS increases.

<<identify>>=
z <- identifyStartSites(yFit)
z
head(segments(z))
head(tss(z, 3))
head(reads(z, 3))
@

<<plot_identify, fig=TRUE, echo=TRUE>>=
plot(z, 3)
@


\section{Customizing figures}

The \Rmethod{plot} method allows for a simple, but powerful customization
of the produced figures. To each element of the graphic, all possible
arguments can be set, supplying them in the form of named lists. In
the following, we omit the the plotting of the threshold and the ratio
estimates, as well as adapt the representation of some components.
For a detailed description on the individual settings, please refer
to the \Rmethod{plot} documentation of this package.


<<plot_custom, fig=TRUE, echo=TRUE>>=
plot(z, 4,
ratio=FALSE,
threshold=FALSE,
baseline=FALSE,
expect=TRUE, expectArgs=list(type="l"), extend=TRUE,
countsArgs=list(type="h", col="darkgray", pch=NA),
plotArgs=list(xlab="Genomic position", main="TSS for segment 's1_-_155'"))
@


\section{Converting and exporting results}

While the get methods \Rmethod{reads}, \Rmethod{segments}, and \Rmethod{tss}
provide a simple access to relevant results, such data can also be
represented with the framework provided by the \Rpackage{IRanges}
package. Converting the data to an object of class \Rclass{RangedData}
allows for a standard representation and interface to other formats,
for example using the \Rpackage{rtracklayer} package.

<<convert_iranges>>=
readsRd <- readsAsRangedData(z)
segmentsRd <- segmentsAsRangedData(z)
tssRd <- tssAsRangedData(z)
tssRd
@
<<export_rtracklayer>>=
library(rtracklayer)
tmpFile <- tempfile()
export.gff3(readsRd, paste(tmpFile, "gff", sep="."))
export.bed(segmentsRd, paste(tmpFile, "bed", sep="."))
export.bed(tssRd, paste(tmpFile, "bed", sep="."))
@

\newpage{}


\section*{Session info}

<<sessionInfo, results=tex, echo=FALSE>>=
toLatex(sessionInfo(), locale=FALSE)
@

\end{document}
